# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

# training attentive nas models with "BestUp-3 (loss)"
#### models ####
arch: 'attentive_nas_dynamic_model'

exp_name: "attentive_nas_dynamic_model_bestup3"

batch_size: 384
batch_size_per_gpu: 384
sandwich_rule: True

grad_clip_value: 1.0

sampler:
    method: 'bestup'
    arch_to_flops_map_file_path: './attentive_nas_data/flops_archs_off_table.map'
    discretize_step: 25
    num_trials: 3

augment: "auto_augment_tf"

n_gpu_per_node: 3
num_nodes: 1
n_cpu_per_node: 32
memory_per_node: '128g'

# TODO:
warmup_epochs: 50
epochs: 700
start_epoch: 0

alpha_min: -1.0
alpha_max: 1.0
iw_clip: 5.0

label_smoothing: 0.2
inplace_distill: True

#sync-batchnormalization, suggested to use in bignas
sync_bn: False

bn_momentum: 0
bn_eps: 1e-5

post_bn_calibration_batch_num: 384

num_arch_training: 4

# TODO:
models_save_dir: "/mnt/HD_2/shh/Alphanet_adder/finetune/no_constraint_scratch"

#### cloud training resources  ####
data_loader_workers_per_gpu: 2

########### regularization ################
dropout: 0.2
drop_connect: 0.2
drop_connect_only_last_two_stages: True
# dropout: 0
# drop_connect: 0
# drop_connect_only_last_two_stages: False

# TODO:
weight_decay_weight: 0.0005
weight_decay_bn_bias: 0.


# TODO:
## =================== optimizer and scheduler======================== #
# optimizer:  
#     method: sgd
#     momentum: 0.9
#     lr: 0.2
#     nesterov: True

# lr_scheduler:
#     method: "cosine"
#     eta_min: 0.002
#     # method: "multistep"
#     milestones: [100,150]
#     gamma: 0.5


optimizer:  
    method: sgd
    momentum: 0.9
    nesterov: True

lr_scheduler:
    method: "warmup_cosine_lr"
    # TODO:
    base_lr: 0.3
    clamp_lr_percent: 0.005


### distributed training settings ###
multiprocessing_distributed: False
dist_backend: 'nccl'
distributed: False


### imagenet dataset ###
# dataset: 'imagenet'
# dataset_dir: "/data/imagenet"
# n_classes: 1000
# drop_last: True
# TODO:
dataset: 'cifar100'
dataset_dir: "/mnt/HD_1/datasets/CIFAR100"
n_classes: 100
drop_last: True

print_freq: 10
# TODO:
resume: "/mnt/HD_2/shh/Alphanet_adder/train/adder_cifar100_pro_pro_2/attentive_nas_dynamic_model_bestup3/alphanet.pth.tar"
# resume: ""

seed: 0

#attentive nas search space
# c: channels, d: layers, k: kernel size, t: expand ratio, s: stride, act: activation, se: se layer
supernet_config:
    use_v3_head: True
    # TODO
    resolutions: [32]
    first_conv: 
        c: [16, 24]
        act_func: 'swish'
        s: 1
    mb1:
        c: [16, 24]
        d: [1, 2]
        k: [3, 5]
        t: [1]
        s: 1
        # act_func: 'swish'
        act_func: 'swish'
        se: False
        # TODO:
        type: ['conv', 'add']
    mb2:
        c: [24, 32]
        d: [3, 4, 5]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
        type: ['conv', 'add']
    mb3:
        c: [32, 40] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: True
        type: ['conv', 'add']
    mb4:
        c: [64, 72] 
        d: [3, 4, 5, 6]
        k: [3, 5]
        t: [4, 5, 6]
        s: 2
        act_func: 'swish'
        se: False
        type: ['conv', 'add']
    mb5:
        c: [112, 120, 128] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [4, 5, 6]
        s: 1
        act_func: 'swish'
        se: True
        type: ['conv', 'add']
    mb6:
        c: [192, 200, 208, 216] 
        d: [3, 4, 5, 6, 7, 8]
        k: [3, 5]
        t: [6]
        s: 2
        act_func: 'swish'
        se: True
        type: ['conv', 'add']
    mb7:
        c: [216, 224] 
        d: [1, 2]
        k: [3, 5]
        t: [6]
        s: 1
        act_func: 'swish'
        se: True
        type: ['conv', 'add']
    last_conv:
        # TODO:
        c: [1792, 1984]
        # c: [1504]
        act_func: 'swish'




